# 儿童预知教育系统

## 1- 逻辑架构图

![image-20210719213849871](images/image-20210719213849871.png)

## 2- 数据流程图

![image-20210719214125619](images/image-20210719214125619.png)

### 2-1数据流程

- 访问咨询看板
  - 客户访问网站----记录访问信息；
  - 客户咨询在线客服----记录咨询数据；
  - 如何区分访问客户还是咨询客户？ [如果客户有咨询信息就是咨询客户，如果没有则是访问客户]()；
- 意向客户看板
  - 客户学习意向信息，比如：学习英语、吉他；
- 有效线索看板
  - 客户留了电话、微信、QQ 等有效的联系方式； 
- 报名看板
  - 客户报名的数据； （付款记录）
- 学员出勤看板
  - 学员出勤数据；

## 3- 项目描述

![image-20210220104549438](images/image-20210220104549438.png)

### 3-1 项目背景

​		由于现在教育行业越来越火，很多教育机构为了更好的扩展市场，开展新的教育方向，提高教育服务。各个教育平台开始对他们的用户进行深度的数据分析和挖掘。但是现在各种教育机构普遍存在一些问题， 系统多，数据量大/分散, 统计分析难高等等问题。一个教育机构分为各种系统比如有网站系统，报名系统，出勤系统等等，还有访问网站的一些日志信息，咨询信息，等等数据非常多而且很杂乱。使用传统的数据库已经无法满足平台统计分析；

### 3-2 为什么会有这个项目？

- 基于五个主题 去分析出客户一些行为画像；
- 基于五大主题了解市场主流课程；
- 基于五大主题了解各个地区的分布情况；
- 从而更好的为公司提供有效的数据作出决策；
- 扩展市场，开展新的教育方向，提高教育服务；

### 3-3 开发过程介绍

#### 3-3-1 数据源：

- ​	电咨、线下面授、在线教育等OLTP系统的数据，大多存储于mysql。

#### 3-3-2 数据抽取：

- 使用sqoop实现关系型数据库和大数据集群的双向同步。

#### 3-3-3 数据存储：

- HDFS

#### 3-3-4 数据清洗：

- 数据的清洗、转换、统计分析等都是使用基于CM管理的Hive来进行的。

#### 3-3-5 数据分析：

- 数据的清洗、转换、统计分析等都是使用基于CM管理的Hive来进行的。

#### 3-3-6 数据同步：

- 使用sqoop实现关系型数据库和大数据集群的双向同步。

#### 3-3-7 OLAP数据服务：

- 采用常用的Mysql数据库。



### 3-4 业务介绍

#### 3-4-1 访问咨询看板

- 1、**总访问**客户量；
  - 统计指定时间段内，[访问客户的总数量]()。能够下钻到小时数据
- 2、**地区**独立访客热力图；
  - 访问客户中[各区域人数]()热力图。能够下钻到小时数据。
- 3、访客**咨询率**趋势；
  - [咨询率=发起咨询的人数/访问客户量]()
- 4、客户访问量和访客**咨询率双轴趋势**；
  - [每日客户访问量/咨询率双轴趋势图]()
- 5、**时间段**访问客户量趋势；
  - [1-24h之间，每个时间段的访问客户量]()
- 6、**来源渠道**访问量占比；
  - [不同来源渠道的访问客户量占比]()；
- 7、**搜索来源**访问量占比；
  - [不同搜索来源的访问客户量占比]()
- 8、**活跃页面**排行榜；
  - [产生访问客户量最多的页面排行榜TOPN]()

#### 3-4-2 意向客户看板

- 意向学员**位置**热力图;
  - 新增的意向客户，[所在城市区域人数]()热力图
- **总意向量**;
  - 新增意向客户（包含自己录入的意向客户）[总数]()
- 意向**学科**排名;
  - 意向[学科]()人数排行榜
- 意向**校区**排名;
  - 意向[校区]()人数排行榜
- **来源渠道占比**;
  - 不同来[源渠道]()的意向客户占比
- **意向贡献中心占比**;
  - [各咨询中心产生的意向客户数占比]()

#### 3-4-3 有效线索看板

- 有效线索**转化率**;
  - [有效线索量/咨询量]()
- 有效线索转化率**时间段趋势**;
  - [统计期内，1-24h之间，每个时间段的有效线索转化率]()
- 有效线索量;
  - 新增的咨询客户中，[有效线索的总量]()

#### 3-4-4 报名看板

- 校区报名柱状图;
  - [各校区报名人数分布图]()
- 学科报名柱状图;
  - [各学科报名人数分布图]()
- 总报名量;
  - [已经缴费的]()报名客户[总量]()
- 线上报名量;
  - 总报名量中[来源渠道为线上]()访客渠道的报名总量
- 意向用户报名转化率;
  - [报名人数/意向人数]()
- 有效线索报名转化率;
  - [线上报名量/线上有效线索量]()
- 日报名趋势图;
  - [每天报名人数的趋势图]()
- 校区学科的报名学员TOPn；
  - [各个校区的各个学科排行榜，topN]()
- 来源渠道占比；
  - [不同来源渠道]()的报名学员占比情况；
- 咨询中心报名贡献；
  - [各咨询中心]()的报名学员人数占比情况

#### 3-4-5 学员出勤看板

- 班级出勤人数；
- 班级出勤率；
- 班级迟到人数；
- 班级迟到率；
- 班级请假人数；
- 班级请假率；
- 班级旷课人数；
- 班级旷课率；



## 4- 个人项目中负责的任务

- 项目初期参加架构设计和讨论；
- 基于sqoop 技术将数据源从mysql导入HDFS;
- 基于Hive技术 完成 **访问咨询主题看板**、**意向优化咨询主题看板**、**有效线索主题看板** ；
  - 需求分析；
  - 建模分析；
  - 数据采集-数据清洗-数据统计分析；
- 基于sqoop 技术将数据源从HDFS导出mysql;
- 后期优化工作；

## 5- 项目中的亮点

### 5-1 ClouderaManager

#### 5-1-1 介绍

- Cloudera Manager是用于管理CDH群集的B/S应用程序；
- 优点：
  - 统一化的**可视化界面**；
  - **自动安装部署和配置**；
  - 自带实时**监控系统**；
  - 大数据各类组件(hadoop、hive、hue、kudu、impala、zookeeper等)安装、调优极其便捷 零停机维护
  - 多个组件**依赖关系不用管**； 
  - 多用户管理；
  - 稳定性好；
- 缺点：
  - 收费；
  - 免费版本不支持弹性升级；

#### 5-1-2 架构

![image-20210719224627046](images/image-20210719224627046.png)

- **Server**：
  - Cloudera Manager的**[核心]()**是Cloudera Manager Server;
  - 提供了[**统一的UI和API**]()方便用户和集群上的CDH以及其它服务进行交互；
  - 能够[安装配置CDH和其相关的服务软件]()；
  - 启动停止服务，维护集群中各个节点服务器以及上面运行的进程。

- **Agent**
  - 安装在每台主机上的[**代理服务**]()。它负责[启动和停止进程，解压缩配置]()，触发[安装和监控]()主机;
- **Management Service：**
  - 执行各种[监控、报警和报告]()功能的一组角色的服务;
- **Database**：
  - CM自身使用的[数据库]()，[存储配置和监控信息]();
- **Cloudera Repository**：
  - [云端存储库]()，提供可供Cloudera Manager分配的软件;
- **Client**：
  - 用于与服务器进行交互的接口，包含[**Admin Console和API;**]()
  - Admin Console：管理员[可视化控制台]() ;
  - API：开发人员使用[API可以创建自定义的Cloudera Manager应用程序]();



### 5-2 Sqoop

- sqoop是一款apache开源产品, 主要是用于**数据的导入导出**的工具;
  - 导入: 从RDBMS到hadoop  
  - 导出: 从Hadoop到RDBMS

#### 5-3-4-1 Sqoop抽取的两种方式

- Sqoop API
- HCatalog API

- 区别

  - Sqoop方式支持的数据格式较少，<span style="color:red;background:white;font-size:20px;font-family:楷体;">**HCatalog支持的数据格式多，包括RCFile, ORCFile, CSV, JSON和SequenceFile等格式。**</span>

  - Sqoop方式**允许数据覆盖**，[HCatalog不允许数据覆盖，每次都只是追加]()。

  - Sqoop方式比较随意，不要求源表和目标表字段相同(字段名称和个数都可以不相同)，<span style="color:blue;background:white;font-size:20px;font-family:楷体;">**它抽取的方式是将字段按顺序插入，**</span>比如目标表有3个字段，源表有一个字段，它会将数据插入到Hive表的第一个字段，其余字段为NULL。

    但是HCatalog不同，<span style="color:red;background:white;font-size:20px;font-family:楷体;">**源表和目标表字段名需要相同，字段个数可以不相等，如果字段名不同，抽取数据的时候会报NullPointerException错误。**</span>HCatalog抽取数据时，会将字段对应到相同字段名的字段上，哪怕字段个数不相等。

  

### 5-3 Hive

- Hive是基于Hadoop的一个[**数据仓库工具**](),可以将[结构化的数据文件映射为一张数据库表]()，并提供类SQL查询功能。

- 其本质是将[SQL转换为MapReduce的任务进行运算]()，底层由HDFS来提供数据的存储，说白了hive可以理解为一个将SQL转换为MapReduce的任务的工具，甚至更进一步可以说hive就是一个MapReduce的客户端。

![image-20210726140631313](images/image-20210726140631313.png)



- 为什么使用Hive

  1、操作接口**采用类SQL语法**，提供快速开发的能力;

  2、**避免了去写MapReduce**，减少开发人员的学习成本;

  3、**功能扩展很方便**;

- Hive的特点

  - 适用于**离线数据分析**；
  - Hive是**将数据映射成数据库和一张张的表**，[库和表的**元数据**信息一般存在关系型数据库上]()（比如MySQL）
  - Hive最大的特点是通过[类SQL来分析大数据]()，而[避免了写MapReduce程序来分析数据]()，这样使得分析数据更容易；
  - **数据存储**：[HDFS]()
  - **数据计算**：[MapReduce]()



### 5-4 ozzie

- *Oozie* 是一个开源的[工作流和协作服务引擎]()

- 工作流具有特点:

  * 1) 整个流程可以被分为**多个节点；**
  * 2) 各个节点**存在依赖关系**(前序没有干, 后续就无法执行)；
  * 3) 此流程需要**周而复始**不断的干；
  * 总结: 如果以后发现有个任务满足了以上三个特征, 那么就可以尝试使用工作流来解决；

- 请问, 大数据的开发流程, 是否可以使用工作流呢?  

  - 回答：一定是可以的

  ![image-20210221102256575](images/image-20210221102256575.png)

  

#### 5-4-1 对比Oozie 与 Azkaban

* **单独使用角度:  azkaban 比 oozie 更加的方便**

  * azkaban: 是由**领英**提供的一款工作流的调度工具, 其内部主要采用通过[配置类似于properties文件来定义工作流](), 此定义方式更加的方便,  而且领英宣称 只要是能够使用shell命令执行的软件, 统统都可以使用azkaban完成调度, [所以说azkaban本质上就是一个shell调度客户端]()
  * oozie: apache旗下的, 出现时间较早, 是大数据一款工作流的调度工具, [oozie采用xml方式来完成工作流调度](), oozie在单独使用下, [**配置是极其复杂**]()的, 而且[oozie所提供管理界面 异常难用, 比较慢]();

  

  [**所以说, 从单独使用角度上, azkaban前景要优于oozie**]()

* **如果是结合HUE 来使用呢?** 

  * azkaban: 由于azkaban是由领英公司开源, 并不属于apache环境, 所以HUE对其是不支持的, 如果需要使用azkaban, [需要单独维护一套azkaban调度环境]()
  * ooize: 是由apache开源, 属于apache怀抱下产品, 同时HUE也是apache旗下的, HUE如果要整合一款调度工具, 必然会选择自家产品 , [所以HUE是可以直接和oozie进行整合, 整合之后, 用户只需要通过鼠标点一点方式即可完成工作流的配置]()

  此时: [ooize要比azkaban更加方便,同时也是同源产品, 兼容性也会更好, 不需要单独维护]()





## 6- 项目中的难点以及解决策略

- 当时完全不懂Hive框架，通过自己在网上找资料主动学习慢慢去了解；

- Hive 基本优化

  ``` properties
  1- 并行查询优化；
  2- 不同阶段并行优化；
  3- 小文件合并优化； 
  4- 批量操作优化；
  5- 零拷贝优化(只读有用字段)；
  ```

- 数据倾斜优化

  ``` properties
  1- join优化;
  2- groupby优化;
  3- union all 优化;
  4- 关联优化器;
  ```

- Hive 的索引优化

  ``` properties
  1- 原始索引;
  2- 行组索引;
  3- 开发过滤索引;
  ```

  









## 7- 项目可以优化点

- 可以基于Spark框架来实现该项目的ETL部分；
- 可以基于kudu + Impala + Hue 实现一个即席查询的环境；





## 8- 项目问题回答

项目部分：最近的这个项目，问自己20个问题并提出回答思路？

1. CDH版本？

2. 项目介绍？

3. 个人职责？

4. 开发环境或生产环境集群节点部署情况？（开发环境是什么样的？）

5. 数据源来源什么?

6. 数据如何导入的？

7. 数据导入工具除了sqoop工具，你还熟悉那些

   ​	kettle、ogg、canal、datax、flume

8. 为什么要分全量和增量导入数据

9. hdfs数据存储格式

10. hive的分区和分桶

11. hive的拉链表

12. hive的自定义函数

    udf：一进一出

    udtf：一进多出（左视图函数）

    udaf：多进一出(聚合函数：max、avg、count)

13. 项目业务流程是什么？

14. 数据模型建立，为什么要这么建立模型？

15. 数仓模型中，DWB存储什么类型数据，分层依据是什么？

16. 任务调度是如何调度？

``` properties
(1) 周期性调度：全量表数据导入任务（dwd），按时间编排任务，14个任务每月调度一次任务

① 员工信息、部分信息数据、油站[新增]信息数据

② 时间编排：t+1调度任务，每月的1号，过了00点30，执行导入地理位置任务数据；

③ 每周的周1,过了00点35，执行导入油站信息数据；每周的周1，过了00点40，执行导入服务商数据；每周的周1，过了00点45，指向导入客户信息数据

(2) 每天执行调度任务：(每天02-04点执行前一天的导入数据任务)总的工单任务、安装单任务、维修单任务，仓储物料任务、设备信息任务
```



17. shell语言熟悉程度？

18. spark的shuffle过程？

19. 除了sparksql，还熟悉什么内容？

20. spark的常用算子，你熟悉那些？

21. flume的数据源种类，数据输出的种类？