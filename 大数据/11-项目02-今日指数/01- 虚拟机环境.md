[TOC]



# 导入虚拟机



​		此虚拟机可直接使用；所有大数据环境已配置完成，直接启动服务，使用即可。

```
账号和密码：
	root/hadoop
```

- 1）、打开VMWare虚拟软件，找到虚拟机目录

![](images/image-20200908130712727.png)



- 2)检查内存、cpu配置

![](images/image-20200908131645595.png)

- 3）、配置VMWare Net8网卡网络地址，保持一致

  ​	![](images/image-20200908131826775.png)

​		修改子网IP地址段，如下所示：

![](images/image-20200908132103949.png)

- 4）、点击启动虚拟机，弹出会话框：【已移动此虚拟机】和【已复制此虚拟机】，**选择【已移动】**。

  ![image-20200908134423557](images/image-20200908134423557.png)

  ​	使用远程客户端连接虚拟机：

  ```properties
  IP地址：
  	192.168.52.100      node01
  
  账号和密码：
  	root/hadoop
  	
  MySQL数据库账号和密码：
  root/123456
  ```

  

![](images/image-20200908133035551.png)

- 5）、配置主机名与IP地址映射
  - 在Windows中配置映射，配置路径：**C:\Windows\System32\drivers\etc\hosts**



![](images/image-20200908132354504.png)

​		内容如下：

```ini
192.168.52.100 node01.hadoop.com node01   
```



- 6）、使用MySQL数据库客户端连接数据库

  （1）连接mysql

![](images/image-20200908133142029.png)

​	（2）查询数据库表

![](images/image-20200908133243457.png)



- 7）关闭虚拟机

  ~~~	shell
  halt
  ~~~

  

## 启动hadoop + yarn

``` properties
#启动hadoop(hdfs/yarn)
start-all.sh
```



## 启动Yarn history server

``` properties
mr-jobhistory-daemon.sh start historyserver
```





## 启动Zookeeper

``` properties
/export/servers/zookeeper-3.4.5-cdh5.14.0/bin/zkServer.sh start
```



## 启动kafka

``` properties
nohup /export/servers/kafka_2.11-1.0.0/bin/kafka-server-start.sh /export/servers/kafka_2.11-1.0.0/config/server.properties >/dev/nul* 2>&1 &

```

``` properties
# 查看所有topic
./kafka-topics.sh  --list --zookeeper node01:2181

# 创建topic
./kafka-topics.sh  --create  --zookeeper node01:2181 --topic sse --partitions 1  --replication-factor 1

# 删除一个topic
kafka-topics.sh  --delete  --zookeeper node01:2181 --topic sse

./kafka-topics.sh  --create  --zookeeper node01:2181 --topic sse --partitions 1  --replication-factor 1

# 生产者
kafka-console-producer.sh  --broker-list node01:9092 --topic szse

# 消费者
kafka-console-consumer.sh  --bootstrap-server node01:9092 --topic szse
```



## FTP服务器

注意：FTP 详细安装方法请查看文档：

F:\博学谷\大数据\大数据进阶课程\07-张红保-今日指数项目\今日指数-210130\第2章\5.扩展\FTP安装.docx

``` properties
# FTP服务器是开启启动的， 用下面命令可以查看。

# 方法一:
[root@node01 ftptest]# ps -ef | grep vsftpd
root       1961      1  0 10:25 ?        00:00:00 /usr/sbin/vsftpd /etc/vsftpd/vsftpd.conf
root       5685   2551  0 14:10 pts/0    00:00:00 grep vsftpd
[root@node01 ftptest]# 

# 方法二:
[root@node01 vsftpd]# service vsftpd status
vsftpd (pid 1961) is running...
[root@node01 vsftpd]# 

```



## 访问FTP

```properties
ftp://node01
ftp://ftptest:ftptest@node01  
ftp://node01 #需要再输入用户名和密码
用户名:ftptest
密码  :ftptest
```





## 沪市模拟数据生成

``` properties
# java -jar /export/servers/tmp/jar/sse-server.jar
```





##  启动Flume

注意：node01 系统中有两个flume

- /export/servers/flume-old  : 可以启动 sse-ftp-source.conf
- /export/servers/flume-1.6.0-cdh5.14.0-bin ： 可以启动console-logger.conf



``` properties
cd /export/servers/flume

bin/flume-ng agent -c conf -f conf/sse-ftp-source.conf -n client -Dflume.root.logger=INFO,console
```





## 启动hive 

``` properties
#启动metastore
cd /export/servers/hive-1.1.0-cdh5.14.0/bin/
nohup hive --service metastore &

#启动 hiverserver2
cd /export/servers/hive-1.1.0-cdh5.14.0/bin/
nohup hive --service hiveserver2 &
```



## 启动beeline

``` properties
cd /export/servers/hive-1.1.0-cdh5.14.0/bin/
beeline
!connect jdbc:hive2://node01:10000
#userName  : root
#passWord  : hadoop
```



## 启动 oozie (需要先启动yarn-history)

``` properties
# 需要先启动 yarn historyserver
mr-jobhistory-daemon.sh start historyserver

# 启动oozie
cd  /export/servers/oozie-4.1.0-cdh5.14.0/bin
./oozied.sh start


# Bootstrap就是oozie的进程
[root@node01 bin]# jps
14336 Bootstrap
14434 Jps
3939 NodeManager
13898 RunJar
2891 NameNode
3628 ResourceManager
4270 JobHistoryServer
4562 RunJar
2645 QuorumPeerMain
3445 SecondaryNameNode
4374 RunJar
3128 DataNode
4767 RunJar
[root@node01 bin]# 
```

## 关闭oozie

``` properties
cd  /export/servers/oozie-4.1.0-cdh5.14.0/bin

# 关闭命令
oozied.sh stop
```



## oozie命令总结

``` properties
cd /export/servers/oozie-4.1.0-cdh5.14.0
#启动命令
bin/oozied.sh start 

#关闭命令
bin/oozied.sh stop

#页面访问
http://node01:11000/oozie/

#查看信息
oozie job -oozie http://node01:11000/oozie -info 0000029-191027171933033-oozie-root-C

#查看日志
oozie job -oozie http://node01:11000/oozie -log 0000064-190923225831711-oozie-root-C

#Kill任务
oozie job -oozie http://node01:11000/oozie -kill 0000064-190923225831711-oozie-root-C
oozie job -kill 0000001-190810214100080-oozie-root-W 

# 查看所有普通任务
oozie  jobs

#查看定时任务
oozie jobs -jobtype coordinator -oozie http://node01:11000/oozie或bin/oozie jobs -jobtype coordinator

```



## 启动 hue

``` properties
cd /export/servers/hue-3.9.0-cdh5.14.0/
build/env/bin/supervisor

#web 访问: node01:8888

用户名: root
密  码: 123456

[root@node01 ~]# ps -ef | grep hue                 
root      14504  13661  0 13:10 pts/4    00:00:00 /export/servers/hue-3.9.0-cdh5.14.0/build/env/bin/python2.6 build/env/bin/supervisor

root      14510  14504  5 13:10 pts/4    00:00:08 /export/servers/hue-3.9.0-cdh5.14.0/build/env/bin/python2.6 /export/servers/hue-3.9.0-cdh5.14.0/build/env/bin/hue runcherrypyserver
root      14708  13762  0 13:12 pts/5    00:00:00 grep hue
[root@node01 ~]# 
```



## 启动HBase

``` properties
[root@node01 bin]# cd /export/servers/hbase-1.1.1/bin
[root@node01 bin]#/export/servers/hbase-1.1.1/bin/start-hbase.sh

# jps
HMaster
HRegionServer

# 进入hbase 
[root@node01 bin]# /export/servers/hbase-1.1.1/bin/hbase shell
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/export/servers/hbase-1.1.1/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/export/servers/hadoop-2.6.0-cdh5.14.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
HBase Shell; enter 'help<RETURN>' for list of supported commands.
Type "exit<RETURN>" to leave the HBase Shell
Version 1.1.1, rd0a115a7267f54e01c72c603ec53e91ec418292f, Tue Jun 23 14:44:07 PDT 2015

1.8.7-p357 :001 > 

```



## 关闭HBase

``` properties
[root@node01 bin]# cd /export/servers/hbase-1.1.1/bin
[root@node01 bin]# stop-hbase.sh
```









## 启动Redis

``` properties
# cd /export/servers/redis-3.2.8/cluster

# cd 7001
redis-server redis.conf
# cd 7002
redis-server redis.conf
# cd 7003
redis-server redis.conf

# 查看进程
ps -ef | grep redis 

[root@node01 7003]# ps -ef | grep redis
root      18945      1  0 17:30 ?        00:00:00 redis-server node01:7001 [cluster]
root      18962      1  0 17:31 ?        00:00:00 redis-server node01:7002 [cluster]
root      18976      1  1 17:31 ?        00:00:00 redis-server node01:7003 [cluster]
root      18982  13042  0 17:31 pts/2    00:00:00 grep redis
```





## Druid

### 启动Druid

``` properties

> 此服务是采用前端方式启动，关闭服务:ctrl+c
> 启动此服务之前，一定要先启动zookeeper

[root@node01 imply-3.0.4]# cd /export/servers/imply-3.0.4/
[root@node01 imply-3.0.4]# nohup bin/supervise -c conf/supervise/quickstart.conf > quickstart.log &

​
# 或者( --daemonize 代表后台启动)
[root@node01 imply-3.0.4]# /export/servers/imply-3.0.4/bin/supervise -c /export/servers/imply-3.0.4/conf/supervise/quickstart.conf --daemonize

[root@node01 ~]# jps
3744 ResourceManager
2856 QuorumPeerMain
3276 DataNode
3085 NameNode
4659 Main
4660 Main
4661 Main
4662 Main
4663 Main
4664 Main
3576 SecondaryNameNode
6107 Jps
3933 NodeManager
4286 Kafka
[root@node01 ~]# 
```



### Web 访问Druid

``` properties
web访问:

存储服务：
http://node01:8081/index.html#/

索引服务：
http://node01:8090/console.html

查询服务：
http://node01:8888/
```





### 停止Druid

``` properties
[root@node01 imply-3.0.4]# /export/servers/imply-3.0.4/bin/service --down
```



### 开启Druid摄取进程

![image-20210527104827736](images/image-20210527104827736.png)



### 停止摄取进程

![image-20210527105008326](images/image-20210527105008326.png)

### 清空Druid中表的数据

- 注意： 
  - 在清空数据之前 ， 必须先**停止摄取进程**；
  - 清空后需要重启Druid;

- 方法一：web 方法

<img src="images/image-20210527105217123.png" alt="image-20210527105217123" style="zoom:150%;" />

- 方法二：物理删除

![image-20210527105340207](images/image-20210527105340207.png)





## Kylin启动

### 启动kylin

``` properties
cd /export/servers/apache-kylin-2.6.3-bin-hbase1x/bin

./kylin.sh start
```





### 启动集群

``` properties
1、启动zookeeper
2、启动HDFS
3、启动YARN集群
4、启动HBase集群
5、启动 metastore
nohup hive --service metastore &
6、启动 hiverserver2
nohup hive --service hiveserver2 &
7、启动Yarn history server
mr-jobhistory-daemon.sh start historyserver
8、启动kylin
./kylin.sh start
9、登录Kylin
http://node01:7070/kylin

10、启动beeline
cd /export/servers/hive-1.1.0-cdh5.14.0/bin/
beeline
!connect jdbc:hive2://node01:10000
#userName  : root
#passWord  : hadoop

用户名:ADMIN
密码:KYLIN
```

