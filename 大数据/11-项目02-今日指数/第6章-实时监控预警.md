# 第6章-实时监控预警



# 课程回顾



- 第一章

  - 项目业务基础知识
    - 名称
      - 个股
      - 指数
      - 板块
      - 涨跌幅
      - 振幅
      - 换手率
  - 页面原型
  - 技术架构
    - 数据流程图
    - 技术组件
      - flume，hive,flink,apache druid ,kafka,hbase,hdfs ,CEP,kylin,Spring Boot
  - 业务模块
    - 数据采集（沪深两市）
    - 离线ETL(是为实时流处理服务得)
    - 实时流处理（第3，4，5天）
    - 预警（实时Flink CEP，离线 apache kylin）
    - 应用服务（web 服务（Spring Boot））
  - 数据采集
    - apche avro(高性能序列化得数据传输框架)
      - schema和数据分离
      - api友好（json）
      - 数据结构丰富
        - 8种基本类型
        - 6种复杂类型
    - 深市数据采集
      - 自定义kafka序列化
  - 第二章
    - 沪市数据采集
      - flume自定义source(掌握)
    - 离线ETL
      - 板块成分股（第五章，板块计算得时候用到），每天跑批一次
      - 个股日K线（第五章，K线行情），只在项目第一次部署得时候使用
      - 指数日K线
  - 第三章
    - 个股业务开发（获取水位数据）
      - 反序列化kafka数据
      - 数据过滤
        - 时间（9：30-15：30）
        - 数据（高开低收==0）‘
        - 个股数据过滤
      - 数据合并
    - apache druid
      - 是一个实时数仓平台（kafka,flink,druid）
      - 优点
        - 亚秒级数据响应
        - 简单SQL查询
        - 海量数据存储
      - 缺点
        - 不支持复杂查询（join）
      - bitMap位图索引
        - 数据预聚合
        - 亚秒级数据查询
      - 原理讲解
        - 索引服务
          - 表得创建，销毁和数据摄取
        - 存储服务
          - 逻辑慨念（chunk），是一个时间范围，（类似于分区得概念），一个大的时间范围之内有可能包含数个小得时间段
          - 物理概念
            - segment是真实存储数据得，包含三部分数据：时间列，维度列，指标列
        - 查询服务
  - 第四章
    - 个股
      - 秒级-》hbase
      - 分时->druid
      - 历史数据备份 ->hdfs
      - 涨跌幅行情 ->druid
    - 指数
      - 秒级
      - 分时
      - 数据备份
  - 第五章
    - 板块
      - 秒级
      - 分时
      - 历史数据备份
    - K线
      - 个股
      - 指数
      - 板块

  

# 课程安排

- 预警
  - 实时预警
    - 业务
      - 换手率
      - 涨跌幅
      - 振幅
    - Flink CEP
    - 项目开发
  - 离线预警



# Flink CEP

cep，也叫复杂事件处理（Complex Event Processing库），是一个flink得类库，主要是实现规则匹配。复杂意思是将多个规则组合在一起，称之为复杂事件。

- 优点

  - API友好，使用简单
  - 高性能，高吞吐
  - 数据是动态，查询是静态得（指的是查询规则）

- 缺点

  - 无法动态更新规则

- 开发步骤：

  ``` properties
  1- 构建数据源
  	-设置水位线、事件时间；	
  2- 定义模式规则
  	- 量词
      - 条件表达式
      - 组合模式
  3- 将规则应用到数据流
  4- 获取符合规则的数据
  5- 打印查询到的数据
  
  ```

  

  - 构建数据源

    - 设置水位线、事件时间；

    ``` java
    assignTimestampsAndWatermarks(new BoundedOutOfOrdernessTimestampExtractor<Message>	(Time.seconds(0)) {
            @Override
            public long extractTimestamp(Message element) {
            return element.getEventTime();
    	}
    });
    ```

  - 定义模式规则Pattern.begin()

    - 量词
    - 条件表达式
    - 组合模式

  ``` java
   Pattern<Message, Message> pattern = Pattern.<Message>begin().where().times().oneOrMore().within().....
  ```

  - 将规则应用到数据流 CEP.pattern

  ``` java
  PatternStream<Message> cep = CEP.pattern(source.keyBy(Message::getId), pattern);
  ```

  - 获取符合规则的数据 select 

  ``` java
  SingleOutputStreamOperator<List<Message>> result = cep.select(new PatternSelectFunction<Message, List<Message>>() {
              @Override
              public List<Message> select(Map<String, List<Message>> pattern) throws Exception {
                  //规则匹配到数据以后,需要根据模式名称,查询匹配到的数据
                  List<Message> begin = pattern.get("begin");
                  return begin;
              }
          });
  ```

  - 打印查询到的数据

  ``` java
  result.print("匹配到的数据:::");
  ```

  

## 量词

### 案例开发

~~~java
package cn.itcast;

import cn.itcast.bean.Message;
import org.apache.flink.cep.CEP;
import org.apache.flink.cep.PatternSelectFunction;
import org.apache.flink.cep.PatternStream;
import org.apache.flink.cep.pattern.Pattern;
import org.apache.flink.cep.pattern.conditions.IterativeCondition;
import org.apache.flink.streaming.api.TimeCharacteristic;
import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.streaming.api.functions.timestamps.BoundedOutOfOrdernessTimestampExtractor;
import org.apache.flink.streaming.api.windowing.time.Time;

import java.util.Arrays;
import java.util.List;
import java.util.Map;

/**
 * @Date 2021
 */
public class MaliceUser {
    //需求：
    //用户如果在10s内，输入了TMD 5次，就认为用户为恶意攻击，识别出该用户
    public static void main(String[] args) throws Exception {

        /**
         * 开发步骤：
         *  1.获取流处理执行环境
         *  2.设置事件时间
         *  3.构建数据源
         *  4.定义模式规则
         *  5.将规则应用到数据流
         *  6.获取符合规则的数据
         *  7.打印查询到的数据
         *  8.执行任务
         */
        //1.获取流处理执行环境
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        //2.设置事件时间
        env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);
        // 3.构建数据源
        SingleOutputStreamOperator<Message> waterData = env.fromCollection(Arrays.asList(
                new Message("1", "TMD", 1558430842000L),//2019-05-21 17:27:22
                new Message("1", "TMD", 1558430843000L),//2019-05-21 17:27:23
                new Message("1", "TMD", 1558430845000L),//2019-05-21 17:27:25
                new Message("1", "TMD", 1558430850000L),//2019-05-21 17:27:30
                new Message("1", "TMD", 1558430851000L),//2019-05-21 17:27:30
                new Message("2", "TMD", 1558430851000L),//2019-05-21 17:27:31
                new Message("1", "TMD", 1558430852000L)//2019-05-21 17:27:32
        )).assignTimestampsAndWatermarks(new BoundedOutOfOrdernessTimestampExtractor<Message>(Time.seconds(0)) {
            @Override
            public long extractTimestamp(Message element) {
                return element.getTime();
            }
        });

        //4.定义模式规则
        //用户如果在10s内，输入了TMD 5次
        Pattern<Message, Message> pattern = Pattern.<Message>begin("begin")
                .where(new IterativeCondition<Message>() {
                    @Override
                    public boolean filter(Message value, Context<Message> ctx) throws Exception {
                        return value.getMsg().equals("TMD");
                    }
                }).times(5).within(Time.seconds(10));

        //5.将规则应用到数据流,匹配规则
        PatternStream<Message> cep = CEP.pattern(waterData.keyBy(Message::getUserId), pattern);
        //6.获取符合规则的数据
        cep.select(new PatternSelectFunction<Message, Object>() {
            @Override
            public Object select(Map<String, List<Message>> pattern) throws Exception {
                //获取匹配到得规则数据
                List<Message> begin = pattern.get("begin");
                return begin;
            }
        }).print(); // 7.打印查询到的数据

        //8.执行任务
        env.execute();
    }
}

~~~

> 总结：
>
> 1.使用flink cep需要设置匹配规则，通过Pattern 设置
>
> 2.匹配规则，获取规则数据没通过CEP实现
>
> 3.量词得使用，量词得设置需要跟在规则后面
>
> ```
> .times(5) //匹配5次
> .times(2,3)   //匹配2次-3次          
> .timesOrMore(2) //匹配>= 2次     
> .oneOrMore()  //匹配 >= 1次
> ```

#### bean

~~~java
package cn.itcast.bean;

import lombok.AllArgsConstructor;
import lombok.Data;
import lombok.experimental.NonFinal;

/**
 * @Date 2021
 */
@Data
@AllArgsConstructor
@NonFinal
public class Message {
    //  new Message("1", "TMD", 1558430842000L),//2019-05-21 17:27:22
    private String userId;
    private String msg;
    private Long time;
}

~~~



## 条件表达式

~~~java
package cn.itcast;

import cn.itcast.bean.LoginEvent;
import org.apache.flink.cep.CEP;
import org.apache.flink.cep.PatternSelectFunction;
import org.apache.flink.cep.PatternStream;
import org.apache.flink.cep.pattern.Pattern;
import org.apache.flink.cep.pattern.conditions.IterativeCondition;
import org.apache.flink.cep.pattern.conditions.SimpleCondition;
import org.apache.flink.streaming.api.datastream.DataStream;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;

import java.util.Arrays;
import java.util.List;
import java.util.Map;

/**
 * @Date 2021
 * 条件表达式得使用
 */
public class ConditionDemo {

    //需求：查询匹配用户登陆状态是fail，且失败次数大于8的数据
    public static void main(String[] args) throws Exception {
        /**
         * 开发步骤（java）：
         * 1.获取流处理执行环境
         * 2.设置但并行度
         * 3.加载数据源
         * 4.设置匹配模式连续where，
         * 先匹配状态（多次），再匹配数量
         * 5.匹配数据提取，返回集合
         * 6.数据打印
         * 7.触发执行
         */
        //1.获取流处理执行环境
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        //2.设置但并行度
        env.setParallelism(1);
        //3.加载数据源
        DataStream<LoginEvent> loginEventStream = env.fromCollection(Arrays.asList(
                new LoginEvent("1", "192.168.0.1", "fail", 8),
                new LoginEvent("1", "192.168.0.2", "fail", 9),
                new LoginEvent("1", "192.168.0.3", "fail", 10),
                new LoginEvent("1", "192.168.0.4", "fail", 10),
                new LoginEvent("2", "192.168.10.10", "success", -1),
                new LoginEvent("3", "192.168.10.10", "fail", 5),
                new LoginEvent("3", "192.168.10.11", "fail", 6),
                new LoginEvent("4", "192.168.10.10", "fail", 6),
                new LoginEvent("4", "192.168.10.11", "fail", 7),
                new LoginEvent("4", "192.168.10.12", "fail", 8),
                new LoginEvent("5", "192.168.10.13", "success", 8),
                new LoginEvent("5", "192.168.10.14", "success", 9),
                new LoginEvent("5", "192.168.10.15", "success", 10),
                new LoginEvent("6", "192.168.10.16", "fail", 6),
                new LoginEvent("6", "192.168.10.17", "fail", 8),
                new LoginEvent("7", "192.168.10.18", "fail", 5),
                new LoginEvent("6", "192.168.10.19", "fail", 10),
                new LoginEvent("6", "192.168.10.18", "fail", 9)
        ));

        //4.设置匹配模式连续where，
        //先匹配状态（多次），再匹配数量
        Pattern<LoginEvent, LoginEvent> pattern = Pattern.<LoginEvent>begin("begin")
                .where(new IterativeCondition<LoginEvent>() {
                    @Override
                    public boolean filter(LoginEvent value, Context<LoginEvent> ctx) throws Exception {
                        return value.getStatus().equals("fail");
                    }
                })
//                .times(2)
//                .where(new SimpleCondition<LoginEvent>() {
//                    @Override
//                    public boolean filter(LoginEvent value) throws Exception {
//                        return value.getCount()>8;
//                    }
//                });
                //组合条件
//        .or(new SimpleCondition<LoginEvent>() {
//            @Override
//            public boolean filter(LoginEvent value) throws Exception {
//                return value.getCount()>8;
//            }
//        });
                //停止条件
                .oneOrMore()
                .until(new SimpleCondition<LoginEvent>() {
                    @Override
                    public boolean filter(LoginEvent value) throws Exception {
                        return value.getCount() == 8;
                    }
                });

        //5.匹配数据提取，返回集合
        PatternStream<LoginEvent> cep = CEP.pattern(loginEventStream.keyBy(LoginEvent::getId), pattern);

        //6.数据打印
        cep.select(new PatternSelectFunction<LoginEvent, Object>() {
            @Override
            public Object select(Map<String, List<LoginEvent>> pattern) throws Exception {

                return pattern.get("begin");
            }
        }).print();
        //7.触发执行
        env.execute();
    }
}

~~~

> 迭代条件表达式：IterativeCondition
>
> 简单条件表达式：SimpleCondition
>
> 组合条件表达式：or,是或者得意思
>
> 停止条件表达式：until，前面必须跟上循环条件，例如oneOrMore，表示跳过满足条件得数据

### bean

~~~java
package cn.itcast.bean;

import lombok.AllArgsConstructor;
import lombok.Data;
import lombok.experimental.NonFinal;

/**
 * @Date 2021
 */
@Data
@AllArgsConstructor
@NonFinal
public class LoginEvent {
    //new LoginEvent("1", "192.168.0.1", "fail", 8),
    private String id; //用户id
    private String ip;//用户ip
    private String status;//用户状态
    private int count;//失败次数
}

~~~



## 组合模式

1.consecutive

连续匹配多次数据,中间如果中断，就不匹配

2.allowCombinations

允许组合模式，允许匹配得数据中间出现中断

案例开发

~~~java
package cn.itcast;

import org.apache.flink.api.java.tuple.Tuple3;
import org.apache.flink.cep.CEP;
import org.apache.flink.cep.PatternSelectFunction;
import org.apache.flink.cep.PatternStream;
import org.apache.flink.cep.pattern.Pattern;
import org.apache.flink.cep.pattern.conditions.SimpleCondition;
import org.apache.flink.streaming.api.datastream.DataStreamSource;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;

import java.util.List;
import java.util.Map;

/**
 * @Date 2021
 */
public class ConsecutiveDemo {
    //需求：从数据源中依次提取"c","a","b"元素
    public static void main(String[] args) throws Exception {

        /**
         * 开发步骤（java）：
         * 1.获取流处理执行环境
         * 2.设置但并行度
         * 3.加载数据源
         * 4.设置匹配模式，匹配"c","a","b"
         *   多次匹配"a"：组合模式
         * 5.匹配数据提取Tuple3
         * 6.数据打印
         * 7.触发执行
         */
        //1.获取流处理执行环境
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        //2.设置但并行度
        env.setParallelism(1);
        //3.加载数据源
        DataStreamSource<String> source = env.fromElements("c", "d", "a", "a", "a", "d", "a", "b");
        //4.设置匹配模式，匹配"c","a","b"
        //  多次匹配"a"：组合模式
        Pattern<String, String> pattern = Pattern.<String>begin("begin")
                .where(new SimpleCondition<String>() {
                    @Override
                    public boolean filter(String value) throws Exception {
                        return value.equals("c");
                    }
                }).followedBy("middle")
                .where(new SimpleCondition<String>() {
                    @Override
                    public boolean filter(String value) throws Exception {
                        return value.equals("a");
                    }
                })
                .oneOrMore()
//                .consecutive() //连续匹配多次a,中间如果中断，就不匹配
                .allowCombinations() //允许组合
                .followedBy("end")
                .where(new SimpleCondition<String>() {
                    @Override
                    public boolean filter(String value) throws Exception {
                        return value.equals("b");
                    }
                });

        //5.匹配数据提取Tuple3
        PatternStream<String> cep = CEP.pattern(source, pattern);

        //6.数据打印
        cep.select(new PatternSelectFunction<String, Object>() {
            @Override
            public Object select(Map<String, List<String>> pattern) throws Exception {
                List<String> begin = pattern.get("begin");
                List<String> middle = pattern.get("middle");
                List<String> end = pattern.get("end");

                return Tuple3.of(begin,middle,end);
            }
        }).print();
        //7.触发执行
        env.execute();
    }
}
~~~

## 案例开发

### 案例1-用户登陆

#### 1.需求

过滤出来在2秒内连续登陆失败的用户

#### 2.代码开发

~~~java
package cn.itcast;

import cn.itcast.bean.LoginUser;
import org.apache.flink.cep.CEP;
import org.apache.flink.cep.PatternSelectFunction;
import org.apache.flink.cep.PatternStream;
import org.apache.flink.cep.pattern.Pattern;
import org.apache.flink.cep.pattern.conditions.SimpleCondition;
import org.apache.flink.streaming.api.TimeCharacteristic;
import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.streaming.api.functions.timestamps.BoundedOutOfOrdernessTimestampExtractor;
import org.apache.flink.streaming.api.windowing.time.Time;

import java.util.Arrays;
import java.util.List;
import java.util.Map;

/**
 * @Date 2021
 */
public class LoginFailDemo {

    //过滤出来在2秒内连续登陆失败的用户
    public static void main(String[] args) throws Exception {

        /**
         * 1.获取流处理执行环境
         * 2.设置并行度,设置事件时间
         * 3.加载数据源,提取事件时间
         * 4.定义匹配模式，设置时间长度
         * 5.匹配模式（分组）
         * 6.数据处理
         * 7.打印
         * 8.触发执行
         */
        //1.获取流处理执行环境
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        //2.设置并行度,设置事件时间
        env.setParallelism(1);
        env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);
        //3.加载数据源,提取事件时间
        SingleOutputStreamOperator<LoginUser> source = env.fromCollection(Arrays.asList(
                new LoginUser(1, "192.168.0.1", "fail", 1558430842000L),        //2019-05-21 17:27:22
                new LoginUser(1, "192.168.0.2", "fail", 1558430843000L),        //2019-05-21 17:27:23
                new LoginUser(1, "192.168.0.3", "fail", 1558430844000L),        //2019-05-21 17:27:24
                new LoginUser(2, "192.168.10.10", "success", 1558430845000L)    //2019-05-21 17:27:25
        )).assignTimestampsAndWatermarks(new BoundedOutOfOrdernessTimestampExtractor<LoginUser>(Time.seconds(0)) {
            @Override
            public long extractTimestamp(LoginUser element) {
                return element.getEventTime();
            }
        });

        //4.定义匹配模式，设置时间长度
        //过滤出来在2秒内连续登陆失败的用户
        Pattern<LoginUser, LoginUser> pattern = Pattern.<LoginUser>begin("begin")
                .where(new SimpleCondition<LoginUser>() {
                    @Override
                    public boolean filter(LoginUser value) throws Exception {
                        return value.getStatus().equals("fail");
                    }
                }).next("next") //表示两个模式之前，是严格相邻得
                .where(new SimpleCondition<LoginUser>() {
                    @Override
                    public boolean filter(LoginUser value) throws Exception {
                        return value.getStatus().equals("fail");
                    }
                }).within(Time.seconds(2));

        //5.匹配模式（分组）
        PatternStream<LoginUser> cep = CEP.pattern(source.keyBy(LoginUser::getUserId), pattern);

        //6.数据处理
        //获取匹配到得数据
        cep.select(new PatternSelectFunction<LoginUser, Object>() {
            @Override
            public Object select(Map<String, List<LoginUser>> pattern) throws Exception {
                return pattern.get("begin");
            }
        }).print();

        env.execute();

    }
}

~~~

#### 3.bean

~~~java
package cn.itcast.bean;

import lombok.AllArgsConstructor;
import lombok.Data;
import lombok.NoArgsConstructor;

/**
 * @Date 2021
 */
@Data
@NoArgsConstructor
@AllArgsConstructor
public class LoginUser {
    private int userId;
    private String ip;
    private String status;
    private Long eventTime;
}

~~~



### 案例2-监控市场价格

#### 1.需求

```
商品售价在1分钟之内有连续两次超过预定商品价格阀值就发送告警信息。
```

> 创建topic，生产测试数据，添加redis阀值，请参考教案：“3.3.2.监控市场价格”

#### 2.代码开发

~~~java
package cn.itcast;

import cn.itcast.bean.Product;
import cn.itcast.util.RedisUtil;
import com.alibaba.fastjson.JSON;
import com.alibaba.fastjson.JSONObject;
import org.apache.flink.api.common.serialization.SimpleStringSchema;
import org.apache.flink.cep.CEP;
import org.apache.flink.cep.PatternSelectFunction;
import org.apache.flink.cep.PatternStream;
import org.apache.flink.cep.pattern.Pattern;
import org.apache.flink.cep.pattern.conditions.SimpleCondition;
import org.apache.flink.configuration.Configuration;
import org.apache.flink.streaming.api.TimeCharacteristic;
import org.apache.flink.streaming.api.datastream.DataStreamSource;
import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.streaming.api.functions.ProcessFunction;
import org.apache.flink.streaming.api.functions.timestamps.BoundedOutOfOrdernessTimestampExtractor;
import org.apache.flink.streaming.api.windowing.time.Time;
import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer011;
import org.apache.flink.util.Collector;
import redis.clients.jedis.JedisCluster;

import java.util.List;
import java.util.Map;
import java.util.Properties;

/**
 * @Date 2021
 * 监控市场价格
 */
public class CepMarkets {

    //商品售价在1分钟之内有连续两次超过预定商品价格阀值就发送告警信息。
    public static void main(String[] args) throws Exception {
        /**
         * 1.获取流处理执行环境
         * 2.设置事件时间、并行度
         * 3.整合kafka
         * 4.新建bean对象
         * 5.通过process获取bean,设置status，并提取事件时间
         * 6.定义匹配模式，设置时间长度
         * 7.匹配模式（分组）
         * 8.查询告警数据
         */
        //1.获取流处理执行环境
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        //2.设置事件时间、并行度
        env.setParallelism(1);
        env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);
        //3.整合kafka
        Properties properties = new Properties();
        properties.setProperty("bootstrap.servers", "node01:9092");
        properties.setProperty("group.id", "testCep");

        FlinkKafkaConsumer011<String> kafkaConsumer = new FlinkKafkaConsumer011<>("cep", new SimpleStringSchema(), properties);
        //从头消费
        kafkaConsumer.setStartFromEarliest();
        DataStreamSource<String> source = env.addSource(kafkaConsumer);

        //5.通过process获取bean,设置status，并提取事件时间
        //需要判断实时流种得商品价格与redis中得预警阀值进行比较，大于得话status设置true
        SingleOutputStreamOperator<Product> processData = source.process(new ProcessFunction<String, Product>() {
            Map<String, String> map = null;

            @Override
            public void open(Configuration parameters) throws Exception {
                //获取redis阀值
                JedisCluster jedisCluster = RedisUtil.getJedisCluster();
                map = jedisCluster.hgetAll("product");
            }

            @Override
            public void processElement(String value, Context ctx, Collector<Product> out) throws Exception {

                //将字符串转json
                JSONObject json = JSON.parseObject(value);
                //{"goodsId":100003,"goodsPrice":12,"goodsName":"flour","alias":"面粉","orderTime":1558430855000}
                Double goodsPrice = json.getDouble("goodsPrice"); //实时流中得
                String goodsName = json.getString("goodsName");
                Double threshold = Double.valueOf(map.get(goodsName));//redis中得
                if (goodsPrice > threshold) {
                    out.collect(new Product(
                            json.getLong("goodsId"),
                            goodsPrice,
                            goodsName,
                            json.getString("alias"),
                            json.getLong("orderTime"),
                            true
                    ));
                }
            }
        }).assignTimestampsAndWatermarks(new BoundedOutOfOrdernessTimestampExtractor<Product>(Time.seconds(0)) {
            @Override
            public long extractTimestamp(Product element) {
                return element.getOrderTime();

            }
        });

        //6.定义匹配模式，设置时间长度
        //商品售价在1分钟之内有连续两次超过预定商品价格阀值就发送告警信息。
        Pattern<Product, Product> pattern = Pattern.<Product>begin("begin")
                .where(new SimpleCondition<Product>() {
                    @Override
                    public boolean filter(Product value) throws Exception {
                        return value.getStatus() == true;
                    }
                }).next("next")
                .where(new SimpleCondition<Product>() {
                    @Override
                    public boolean filter(Product value) throws Exception {
                        return value.getStatus() == true;
                    }
                }).within(Time.minutes(1));

        //7.匹配模式（分组）
        PatternStream<Product> cep = CEP.pattern(processData.keyBy(Product::getGoodsId), pattern);
        //8.查询告警数据
        cep.select(new PatternSelectFunction<Product, Object>() {
            @Override
            public Object select(Map<String, List<Product>> pattern) throws Exception {

                return pattern.get("next");
            }
        }).print();

        env.execute();
    }
}
~~~

#### 3.bean

~~~java
package cn.itcast.bean;

import lombok.AllArgsConstructor;
import lombok.Data;
import lombok.NoArgsConstructor;

/**
 * @Date 2021
 */
@Data
@AllArgsConstructor
@NoArgsConstructor
public class Product {

    private Long goodsId;
    private Double goodsPrice;
    private String goodsName;
    private String alias;
    private Long orderTime;
    private Boolean status;

}
~~~



# 实时预警

## 初始化环境

~~~java
package cn.itcast.job;

import cn.itcast.avro.AvroDeserializerSchema;
import cn.itcast.avro.SseAvro;
import cn.itcast.avro.SzseAvro;
import cn.itcast.bean.CleanBean;
import cn.itcast.config.QuotConfig;
import cn.itcast.map.SseMap;
import cn.itcast.map.SzseMap;
import cn.itcast.task.AmplitudeTask;
import cn.itcast.task.UpDownTask;
import cn.itcast.util.QuotUtil;
import org.apache.flink.api.common.functions.FilterFunction;
import org.apache.flink.streaming.api.TimeCharacteristic;
import org.apache.flink.streaming.api.datastream.DataStream;
import org.apache.flink.streaming.api.datastream.DataStreamSource;
import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.streaming.api.functions.timestamps.BoundedOutOfOrdernessTimestampExtractor;
import org.apache.flink.streaming.api.windowing.time.Time;
import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer011;

import java.util.Properties;

/**
 * @Date 2021
 * 预警业务：涨跌幅，振幅，换手率 ，只针对于个股
 */
public class WarnStream {

    //1.创建WarnStream单例对象，创建main方法
    public static void main(String[] args) throws Exception {
        /**
         * 个股总体开发步骤：
         *  1.创建WarnStream单例对象，创建main方法
         *  2.获取流处理执行环境
         *  3.设置事件时间、并行度
         *  4.设置检查点机制
         *  5.设置重启机制
         *  6.整合Kafka(新建反序列化类)
         *  7.数据过滤（时间和null字段）
         *  8.数据转换、合并
         *  9.过滤个股数据
         *  10.设置水位线
         *  11.业务数据处理
         *  12.触发执行
         */
        //2.获取流处理执行环境
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        //3.设置事件时间、并行度
        env.setParallelism(1);//开发环境便于测试，你设置1个，生产环境与kafka的分区数保持一致
        env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);
//        //4.设置检查点机制
//        env.enableCheckpointing(5000l);//发送检查点的时间间隔
//        env.setStateBackend(new FsStateBackend("hdfs://node01:8020/checkpoint/stock"));//状态后端，保存检查点的路径
//        env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);//强一致性，保证数据只会消费一次
//        env.getCheckpointConfig().setCheckpointTimeout(60000l);
//        env.getCheckpointConfig().setFailOnCheckpointingErrors(false);//当检查点制作失败的时候，任务继续运行
//        //当任务取消的时候，保留检查点，缺点是：需要手动删除
//        env.getCheckpointConfig().enableExternalizedCheckpoints(CheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);

        //5.设置重启机制
//        env.setRestartStrategy(RestartStrategies.fixedDelayRestart(3, Time.seconds(5)));

        //6.整合Kafka(新建反序列化类)
        Properties properties = new Properties();
        properties.setProperty("bootstrap.servers", QuotConfig.config.getProperty("bootstrap.servers"));
        properties.setProperty("group.id", QuotConfig.config.getProperty("group.id"));

        //消费kafka数据
        //消费sse
        FlinkKafkaConsumer011<SseAvro> sseKafkaConsumer = new FlinkKafkaConsumer011<SseAvro>(QuotConfig.config.getProperty("sse.topic"), new AvroDeserializerSchema(QuotConfig.config.getProperty("sse.topic")), properties);
        //消费szse
        FlinkKafkaConsumer011<SzseAvro> szseKafkaConsumer = new FlinkKafkaConsumer011<SzseAvro>(QuotConfig.config.getProperty("szse.topic"), new AvroDeserializerSchema(QuotConfig.config.getProperty("szse.topic")), properties);

        sseKafkaConsumer.setStartFromEarliest();
        szseKafkaConsumer.setStartFromEarliest();

        //沪市
        DataStreamSource<SseAvro> sseSource = env.addSource(sseKafkaConsumer);
        //深市
        DataStreamSource<SzseAvro> szseSource = env.addSource(szseKafkaConsumer);

        //7.数据过滤（时间和null字段）
        //null字段，在我们这里就是数据为0的字段
        //沪市过滤
        SingleOutputStreamOperator<SseAvro> sseFilterData = sseSource.filter(new FilterFunction<SseAvro>() {
            @Override
            public boolean filter(SseAvro value) throws Exception {
                return QuotUtil.checkData(value) && QuotUtil.checkTime(value);
            }
        });
        //深市过滤
        SingleOutputStreamOperator<SzseAvro> szseFilterData = szseSource.filter(new FilterFunction<SzseAvro>() {
            @Override
            public boolean filter(SzseAvro value) throws Exception {
                return QuotUtil.checkData(value) && QuotUtil.checkTime(value);
            }
        });

        //8.数据转换、合并
        DataStream<CleanBean> unionData = sseFilterData.map(new SseMap()).union(szseFilterData.map(new SzseMap()));

        //9.过滤个股数据
        SingleOutputStreamOperator<CleanBean> stockData = unionData.filter(new FilterFunction<CleanBean>() {
            @Override
            public boolean filter(CleanBean value) throws Exception {
                return QuotUtil.isStock(value);
            }
        });

        //10.设置水位线
        DataStream<CleanBean> waterData = stockData.assignTimestampsAndWatermarks(new BoundedOutOfOrdernessTimestampExtractor<CleanBean>(Time.seconds(Long.valueOf(QuotConfig.config.getProperty("delay.time")))) {
            @Override
            public long extractTimestamp(CleanBean element) {
                return element.getEventTime();
            }
        });

//        waterData.print("<<预警<<<:");
        /**
         * 1.振幅
         * 2.涨跌幅
         * 3.换手率
         */
        //1.振幅
        //new AmplitudeTask().process(waterData,env);

        //2.涨跌幅
        new UpDownTask().process(waterData);

        //12.触发执行
        env.execute("warn stream");
    }

}

~~~

## 1.振幅

### (1)AmplitudeTask

~~~java
package cn.itcast.task;

import cn.itcast.bean.CleanBean;
import cn.itcast.bean.WarnAmplitudeBean;
import cn.itcast.bean.WarnBaseBean;
import cn.itcast.inter.ProcessDataCepInterface;
import cn.itcast.mail.MailSend;
import cn.itcast.util.RedisUtil;
import org.apache.flink.api.common.functions.MapFunction;
import org.apache.flink.cep.CEP;
import org.apache.flink.cep.PatternSelectFunction;
import org.apache.flink.cep.PatternStream;
import org.apache.flink.cep.pattern.Pattern;
import org.apache.flink.cep.pattern.conditions.SimpleCondition;
import org.apache.flink.streaming.api.datastream.DataStream;
import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.table.api.Table;
import org.apache.flink.table.api.TableEnvironment;
import org.apache.flink.table.api.java.StreamTableEnvironment;
import redis.clients.jedis.JedisCluster;

import java.math.BigDecimal;
import java.util.List;
import java.util.Map;

/**
 * @Date 2021
 * 振幅
 */
public class AmplitudeTask implements ProcessDataCepInterface {
    @Override
    public void process(DataStream<CleanBean> waterData, StreamExecutionEnvironment env) {

        /**
         * 1.数据转换
         * 2.初始化表执行环境
         * 3.注册表（流）
         * 4.sql执行
         * 5.表转流
         * 6.模式匹配,比较阀值
         * 7.查询数据
         * 8.发送告警邮件
         */
        //1.数据转换
        SingleOutputStreamOperator<WarnBaseBean> mapData = waterData.map(new MapFunction<CleanBean, WarnBaseBean>() {
            @Override
            public WarnBaseBean map(CleanBean value) throws Exception {
                //secCode、preClosePrice、highPrice、lowPrice、closePrice、eventTime
                WarnBaseBean warnBaseBean = new WarnBaseBean(
                        value.getSecCode(),
                        value.getPreClosePrice(),
                        value.getMaxPrice(),
                        value.getMinPrice(),
                        value.getTradePrice(),
                        value.getEventTime()
                );
                return warnBaseBean;
            }
        });

        //2.初始化表执行环境
        StreamTableEnvironment tblEnv = TableEnvironment.getTableEnvironment(env);
        //3.注册表（流）
        //振幅： (max(最高点位)-min(最低点位))/期初前收盘点位*100%
        tblEnv.registerDataStream("tbl",mapData,"secCode,preClosePrice,highPrice,lowPrice,eventTime.rowtime");
        //4.sql执行
        //通过sql直接获取振幅结果数据
        String sql = "select secCode,preClosePrice,round((max(highPrice)-min(lowPrice))/preClosePrice,2) as amplitude " +
                " from tbl group by secCode,preClosePrice,tumble(eventTime, interval '5' second)";
        Table table = tblEnv.sqlQuery(sql);
        //5.表转流
        DataStream<WarnAmplitudeBean> resData = tblEnv.toAppendStream(table, WarnAmplitudeBean.class);
//        resData.print("sql查询数据：");

        //6.模式匹配,比较阀值
        //先获取阀值
        JedisCluster jedisCluster = RedisUtil.getJedisCluster();
        BigDecimal amplitudeThreshold = new BigDecimal(jedisCluster.hget("quot", "amplitude"));
        //定义匹配规则
        Pattern<WarnAmplitudeBean, WarnAmplitudeBean> pattern = Pattern.<WarnAmplitudeBean>begin("begin")
                .where(new SimpleCondition<WarnAmplitudeBean>() {
                    @Override
                    public boolean filter(WarnAmplitudeBean value) throws Exception {
                        return value.getAmplitude().compareTo(amplitudeThreshold) == 1;
                    }
                });
        //7.查询数据
        PatternStream<WarnAmplitudeBean> cep = CEP.pattern(resData.keyBy(WarnAmplitudeBean::getSecCode), pattern);
        cep.select(new PatternSelectFunction<WarnAmplitudeBean, Object>() {
            @Override
            public Object select(Map<String, List<WarnAmplitudeBean>> pattern) throws Exception {

                List<WarnAmplitudeBean> list = pattern.get("begin");
                //邮件告警
                if(list != null && list.size() >0){
                    MailSend.send("振幅数据实时预警，详情如下："+list.toString());
                }
                return list;
            }
        }).print();

    }
}

~~~

### (2)WarnBaseBean

~~~java
package cn.itcast.bean;

import lombok.AllArgsConstructor;
import lombok.Data;
import lombok.NoArgsConstructor;

import java.math.BigDecimal;

/**
 * @Date 2021
 */
@Data
@NoArgsConstructor
@AllArgsConstructor
public class WarnBaseBean {

    /**
     * 证券代码
     */
    private String secCode;
    /**
     * 昨收盘价
     */
    private BigDecimal preClosePrice;
    /**
     * 当日最高价
     */
    private BigDecimal highPrice;
    /**
     * 当日最低价
     */
    private BigDecimal lowPrice;
    /**
     * 当日收盘价
     */
    private BigDecimal closePrice;
    /**
     * 事件时间
     */
    private Long eventTime;
}

~~~

### (3)WarnAmplitudeBean

~~~java
package cn.itcast.bean;

import lombok.AllArgsConstructor;
import lombok.Data;
import lombok.NoArgsConstructor;

import java.math.BigDecimal;

/**
 * @Date 2021
 * 接收flink sql计算好的振幅数据
 */
@Data
@AllArgsConstructor
@NoArgsConstructor
public class WarnAmplitudeBean {

    //secCode、preClosePrice、amplitude
    private String secCode;
    private BigDecimal preClosePrice ;
    private BigDecimal amplitude ;//振幅

}
~~~



## 2.涨跌幅

~~~java
package cn.itcast.task;

import cn.itcast.bean.CleanBean;
import cn.itcast.bean.WarnBaseBean;
import cn.itcast.inter.ProcessDataInterface;
import cn.itcast.mail.MailSend;
import cn.itcast.util.RedisUtil;
import org.apache.flink.api.common.functions.MapFunction;
import org.apache.flink.cep.CEP;
import org.apache.flink.cep.PatternSelectFunction;
import org.apache.flink.cep.PatternStream;
import org.apache.flink.cep.pattern.Pattern;
import org.apache.flink.cep.pattern.conditions.SimpleCondition;
import org.apache.flink.streaming.api.datastream.DataStream;
import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;
import redis.clients.jedis.JedisCluster;

import java.math.BigDecimal;
import java.util.List;
import java.util.Map;

/**
 * @Date 2021
 * 实时预警——涨跌幅
 */
public class UpDownTask implements ProcessDataInterface {
    @Override
    public void process(DataStream<CleanBean> waterData) {
        /**
         * 1.数据转换map
         * 2.封装样例类数据
         * 3.加载redis涨跌幅数据
         * 4.模式匹配
         * 5.获取匹配模式流数据
         *   6.查询数据
         * 7.发送告警邮件
         */
        //1.数据转换map
        SingleOutputStreamOperator<WarnBaseBean> mapData = waterData.map(new MapFunction<CleanBean, WarnBaseBean>() {
            @Override
            public WarnBaseBean map(CleanBean value) throws Exception {
                WarnBaseBean warnBaseBean = new WarnBaseBean(
                        value.getSecCode(),
                        value.getPreClosePrice(),
                        value.getMaxPrice(),
                        value.getMinPrice(),
                        value.getTradePrice(),
                        value.getEventTime()
                );
                return warnBaseBean;
            }
        });

        //3.加载redis涨跌幅数据
        JedisCluster jedisCluster = RedisUtil.getJedisCluster();
        //跌幅限制
        BigDecimal upd1 = new BigDecimal(jedisCluster.hget("quot", "upDown1"));
        //涨幅限制
        BigDecimal upd2 = new BigDecimal(jedisCluster.hget("quot", "upDown2"));

        //4.模式匹配
        Pattern<WarnBaseBean, WarnBaseBean> pattern = Pattern.<WarnBaseBean>begin("upDown")
                .where(new SimpleCondition<WarnBaseBean>() {
                    @Override
                    public boolean filter(WarnBaseBean value) throws Exception {

                        //判断涨跌幅大小
                        //涨跌幅：(期末收盘点位-期初前收盘点位)/期初前收盘点位*100%
                        //计算实时流中得涨跌幅
                        BigDecimal updStream = (value.getClosePrice().subtract(value.getPreClosePrice())).divide(value.getPreClosePrice(), 2, BigDecimal.ROUND_HALF_UP);
                        if (updStream.compareTo(upd1) == -1 || updStream.compareTo(upd2) == 1) {
                            return true;
                        }
                        return false;
                    }
                });

        // 5.获取匹配模式流数据
        PatternStream<WarnBaseBean> cep = CEP.pattern(mapData.keyBy(WarnBaseBean::getSecCode), pattern);
        //6.查询数据
        cep.select(new PatternSelectFunction<WarnBaseBean, Object>() {
            @Override
            public Object select(Map<String, List<WarnBaseBean>> pattern) throws Exception {
                List<WarnBaseBean> list = pattern.get("upDown");
                if(list != null && list.size() >0){
                    //发送实时告警邮件
                    MailSend.send("涨跌幅实时告警:"+list.toString());
                }
                return list;
            }
        }).print();

    }
}
~~~




