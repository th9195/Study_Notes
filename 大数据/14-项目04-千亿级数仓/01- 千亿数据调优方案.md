spark on yarn

![SparkOnYarn流程详解](images/SparkOnYarn流程详解.jpg)





一个大表  关联两个小表

关联字段  shop_id   goods_id

数据倾斜  某些partition非常大， 某些partition非常小；

如果是付费版的 Spark 框架 ， 可以直接使用 skew 的方法， 告诉spark 哪个字段会有数据倾斜；

如果是开源的Spark 框架怎么解决？

- 给关联字段加 N 倍的 后缀；
- 让维度表放大N 倍；



如：

事实表 1000w条数据；

id ： 1 2 3 

打散： 每个id 后面随机添加1-100 的随机数据； 

1_1  1_2 1_3  ......  1_100

2_1  2_2 2_3  ......  2_100

3_1  3_2 3_3  ......  3_100



事实表的数据量不变，匹配的id增加随机的后缀， 只是关联的id 变了。





维度表 100 条数据

id  : 1  2  3 

1: 1_1, 1_2,1_3,1_4...... 1_100  放大100倍；

2: 2_1, 2_2,2_3,2_4...... 2_100  放大100倍；

3: 3_1, 3_2,3_3,3_4......3_100  放大100倍；



维度表数据量增大的100 倍，关联的id 也变了。 但是id前缀的一样的数据是一样的；



这样就需要新增三个临时表；

事实表给关联 id 随机添加后缀的临时表；

维度表每个关联 id 扩大100倍的临时表；explode





将添加后缀后的数据再进行关联查询；
